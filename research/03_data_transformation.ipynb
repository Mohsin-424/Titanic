{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Machine_Learning\\\\Titanic_Pipeline_Project\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Machine_Learning\\\\Titanic_Pipeline_Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from titanic.constants import *\n",
    "from titanic.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_file_path = CONFIG_FILE_PATH,\n",
    "            params_file_path = PARAMS_FILE_PATH\n",
    "            ):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_path= config.data_path,\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from titanic import logging\n",
    "from titanic.logging import logger\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from titanic.utils.common import save_object\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def load_dataframe(self):\n",
    "        df = pd.read_csv(os.path.join(self.config.data_path, \"Titanic-Dataset.csv\"))\n",
    "        logger.info(f\"Loaded {df.shape[0]} rows of data\")\n",
    "        return df\n",
    "    \n",
    "    def create_preprocessor(self):\n",
    "\n",
    "        num_col = ['Age']\n",
    "        cat_col = ['Sex','Embarked']\n",
    "\n",
    "        num_pipe = Pipeline(\n",
    "            steps=[\n",
    "                # Handling the missing values\n",
    "                ('imputing_AGE',SimpleImputer(strategy='mean')),\n",
    "                # scaling \n",
    "                ('scalling_data', StandardScaler(with_mean=False)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cat_pipe = Pipeline(\n",
    "            steps=[\n",
    "                # Handling the missing values\n",
    "                ('imputing_AGE',SimpleImputer(strategy='most_frequent')),\n",
    "                # encoding\n",
    "                ('one_hot_encoder',OneHotEncoder(handle_unknown='ignore')),\n",
    "                # scaling\n",
    "                ('scalling_data', StandardScaler(with_mean=False)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            [\n",
    "                ('num_pipeline', num_pipe ,num_col),\n",
    "                ('cat_pipeline', cat_pipe ,cat_col),\n",
    "            ]\n",
    "        )\n",
    "        logger.info(\"Preprocessing pipeline\")\n",
    "        return preprocessor\n",
    "\n",
    "        \n",
    "        \n",
    "    def initiate_pp(self):\n",
    "        preprocessor_obj = self.create_preprocessor()\n",
    "        df = self.load_dataframe()\n",
    "        \n",
    "        x = df.drop('Survived', axis = 1)\n",
    "        y = df['Survived']\n",
    "        \n",
    "        # Train test split\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y, random_state=42, test_size=0.2)\n",
    "\n",
    "        # Preprocessing\n",
    "        x_train = preprocessor_obj.fit_transform(x_train)\n",
    "        \n",
    "        x_test = preprocessor_obj.transform(x_test)\n",
    "\n",
    "        save_object(path = Path(os.path.join(self.config.root_dir,\"preprocessor.pkl\")),obj =preprocessor_obj)\n",
    "        logger.info(\"Preprocessor Saved\")\n",
    "        \n",
    "        return x_train,x_test,y_train,y_test\n",
    "\n",
    "        \n",
    "    \n",
    "    def save_transformed_data(self):\n",
    "        df = self.initiate_pp()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-07 22:52:59,891: INFO: common: yaml file config\\config.yaml loaded successfully]\n",
      "[2024-01-07 22:52:59,895: INFO: common: yaml file params.yaml loaded successfully]\n",
      "[2024-01-07 22:52:59,897: INFO: common: created directory at: artifacts]\n",
      "[2024-01-07 22:52:59,899: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-01-07 22:52:59,900: INFO: 4275834138: Preprocessing pipeline]\n",
      "[2024-01-07 22:52:59,907: INFO: 4275834138: Loaded 891 rows of data]\n",
      "[2024-01-07 22:52:59,927: INFO: 4275834138: Preprocessor Saved]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_tranformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(data_tranformation_config)\n",
    "    data_transformation.save_transformed_data()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
